{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define the Activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(z, a_type, derivative=False):\n",
    "    \n",
    "    '''\n",
    "    This function takes care of calculating the activations during forward propagation\n",
    "    and derivative of activation in the backprop.\n",
    "    \n",
    "    Arguments: \n",
    "    --------------------\n",
    "    Z          : The preactivations\n",
    "    a_type     : Name of the activation function as a string\n",
    "    derivative : True during backprop, False to just calculate activations\n",
    "    \n",
    "    Valid a_types:\n",
    "        * 'sigmoid'\n",
    "        * 'tanh'\n",
    "        * 'relu'\n",
    "        * 'leakyrelu'\n",
    "    \n",
    "    Returns:\n",
    "    --------------------\n",
    "    Activation if derivative = False\n",
    "    dericative if derivative = True\n",
    "    '''\n",
    "    \n",
    "\n",
    "    if a_type == 'sigmoid':\n",
    "        if derivative:\n",
    "            s = activation(z, a_type='sigmoid')\n",
    "            return s*(1-s)\n",
    "            # return the derivative of the sigmoid activation function\n",
    "        else:\n",
    "            return 1/(1+np.exp(-z))\n",
    "            # return the normal sigmoid activation function\n",
    "    \n",
    "    if a_type == 'tanh':\n",
    "        if derivative:\n",
    "            a = activation(z, a_type='tanh')\n",
    "            return (1-a**2)\n",
    "        else:\n",
    "            return np.tanh(z)\n",
    "    \n",
    "    if a_type == 'relu':\n",
    "        if derivative:\n",
    "            return (z > 0) * 1\n",
    "        else:\n",
    "            # fastest way to calculate relu:\n",
    "            return (abs(z) + z) / 2\n",
    "    \n",
    "    if a_type == 'leakyrelu':\n",
    "        if derivative:\n",
    "            return ((z > 0)*1 + (z<=0)*0.2)\n",
    "        else:\n",
    "            return ((z > 0)*z + (z<=0)*0.2*z)\n",
    "    \n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def accuracy(arr1, arr2):\n",
    "    '''\n",
    "    This function calculates how accurate arr2 is to arr1 (fraction of perfect matches)\n",
    "    \n",
    "    Arguments:\n",
    "    ---------------------\n",
    "    arr1: Numpy array1\n",
    "    arr2: Numpy array2\n",
    "    \n",
    "    Returns:\n",
    "    ---------------------\n",
    "    accuracy as described above!\n",
    "    '''\n",
    "    \n",
    "    return(np.sum(arr1==arr2)/len(arr1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define the loss function and a function to find derivative of loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, y_pred, epsilon=0.0000002):\n",
    "    '''\n",
    "    This function returns the element wise cross entropy ( log loss )\n",
    "    \n",
    "    For arrays of size (n, 1), it returns an array of (n, 1)\n",
    "    \n",
    "    To get a single number for loss across all examples do a np.mean() for the output of this function\n",
    "    \n",
    "    Arguments:\n",
    "    -------------------\n",
    "    y_true: The ground truth values (as 0 or 1) wrt which loss has to be found\n",
    "    y_pred: The predictions for which the loss has to be found\n",
    "    \n",
    "    Returns: \n",
    "    -------------------\n",
    "    Element wise log loss\n",
    "    '''\n",
    "    \n",
    "    # clip the y_pred to avoid infinite loss \n",
    "    y_pred = np.clip(y_pred, epsilon, 1-epsilon)\n",
    "    \n",
    "    # reshape np array for simpler calculation\n",
    "    shape = y_true.shape\n",
    "    size = np.prod(shape)\n",
    "    y_true = np.reshape(y_true, (size,))\n",
    "    y_pred = np.reshape(y_pred, (size,))\n",
    "    \n",
    "    # calculate cost using masks\n",
    "    cost = np.zeros(shape)\n",
    "    cost = (y_true==0)*( -1*np.log(1-y_pred) ) + (y_true==1)*( -1*np.log(y_pred) )\n",
    "    return cost\n",
    "\n",
    "def cross_entropy_prime(y_true, y_pred, epsilon=0.0000002):\n",
    "    '''\n",
    "    This function is used to find the derivative of the cross_entropy_loss function\n",
    "    \n",
    "    For arrays of size (n, 1), it returns an array of (n, 1)\n",
    "    \n",
    "    Arguments:\n",
    "    -------------------\n",
    "    y_true: The ground truth values (as 0 or 1) wrt which loss has to be found\n",
    "    y_pred: The predictions for which the loss has to be found\n",
    "    \n",
    "    Returns: \n",
    "    -------------------\n",
    "    derivatives wrt y_true and y_pred \n",
    "    '''\n",
    "    \n",
    "    # clip the y_pred to avoid infinite loss \n",
    "    y_pred = np.clip(y_pred, epsilon, 1-epsilon)\n",
    "    \n",
    "    # save the dimentions:\n",
    "    shape = y_true.shape\n",
    "    size = np.prod(shape)\n",
    "    \n",
    "    # reshape for simpler calculations:\n",
    "    y_true = np.reshape(y_true, (size,))\n",
    "    y_pred = np.reshape(y_pred, (size,))\n",
    "\n",
    "    # Calculate the derivative of the cost function\n",
    "    cost_prime = np.zeros(y_pred.shape)\n",
    "    cost_prime[y_true==0] = 1/(1-y_pred[y_true==0])\n",
    "    cost_prime[y_true==1] = -1/y_pred[y_true==1]\n",
    "    \n",
    "    return cost_prime.reshape(shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write the nn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \n",
    "    '''\n",
    "    This class contains all the functions of the neural network.\n",
    "    Initialize a classic Neural Network by calling the constructor of this class!\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, size, activation = 'relu', out_activation='sigmoid'):\n",
    "        '''\n",
    "        The constructor of this class!\n",
    "        Use this constructor to define the hyperparameters of the nn you want to design!\n",
    "        \n",
    "        Arguments:\n",
    "        ----------------------\n",
    "        size: The shape of a nn. For example a nn with 7 input nodes, then a hidden layer with 5 nodes and a 1 node output\n",
    "              will have the 'size' parameter as: [7, 5, 1]\n",
    "        \n",
    "        activation     : the activations of all the hidden layers as a string\n",
    "        out_activation : the activation of the output layer as a string\n",
    "        \n",
    "        valid activation/ outactivation values:\n",
    "            * 'sigmoid'\n",
    "            * 'tanh'\n",
    "            * 'relu'\n",
    "            * 'leakyrelu'\n",
    "            \n",
    "        Returns: \n",
    "        ----------------------\n",
    "        A neural network object\n",
    "        '''\n",
    "        print('[INFO] initializing network')\n",
    "        self.size = size\n",
    "        self.activation = activation\n",
    "        self.out_activation = out_activation\n",
    "        \n",
    "        # initialzie biases and variance\n",
    "        self.biases = [np.zeros((n, 1)) for n in self.size[1:]]\n",
    "        # use he initialization\n",
    "        self.weights =[np.random.randn(a, b)*np.sqrt(2/(a+b)) for a,b in zip(size[1:], size[:-1])]\n",
    "     \n",
    "    \n",
    "    def forward(self, inpt):\n",
    "        '''\n",
    "        Used in the forward propagation\n",
    "        [NOTE] to calculate just the outputs use calc_output\n",
    "        \n",
    "        Argument:\n",
    "        -----------------\n",
    "        inpt: inputs for the nn *** with number of examples as the second dimention *** \n",
    "        \n",
    "        output:\n",
    "        out_activations, all pre-activations of nn, all acttivations of nn'''\n",
    "        \n",
    "        # do one forward prop\n",
    "        a = inpt\n",
    "        pre_activations = []\n",
    "        activations = [a]\n",
    "        \n",
    "        # all the hidden layers\n",
    "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            z = np.dot(w, a) + b\n",
    "            a  = activation(z, a_type=self.activation)\n",
    "            pre_activations.append(z)\n",
    "            activations.append(a)\n",
    "            \n",
    "        # out layer:\n",
    "        z = np.dot(self.weights[-1], a) + self.biases[-1]\n",
    "        a  = activation(z, a_type=self.out_activation)\n",
    "        pre_activations.append(z)\n",
    "        activations.append(a)\n",
    "        \n",
    "        return a, pre_activations, activations    \n",
    "    \n",
    "    \n",
    "    def calc_output(self, inpt):\n",
    "        '''\n",
    "        Same as the forward function but \n",
    "        doesn't return pre_activations or activations\n",
    "        \n",
    "        Argument:\n",
    "        -----------------\n",
    "        inpt: inputs for the nn *** with number of examples as the second dimention **\n",
    "        \n",
    "        Output:\n",
    "        -----------------\n",
    "        output layer activations\n",
    "        '''\n",
    "        \n",
    "        a = inpt\n",
    "        \n",
    "        # all the hidden layers\n",
    "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            z = np.dot(w, a) + b\n",
    "            a = activation(z, a_type=self.activation)\n",
    "            \n",
    "        # last layer without activation\n",
    "        z = np.dot(self.weights[-1], a) + self.biases[-1]\n",
    "        a = activation(z, a_type=self.out_activation)\n",
    "        \n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def compute_deltas(self, pre_activations, y_true = None, y_pred=None, dis_cache=None):\n",
    "        '''\n",
    "        Compute the deltas of all the nodes used to calculate the derivatives of all the weights and biases\n",
    "        \n",
    "        Working 1:\n",
    "        [use this if this is the topmost nn]\n",
    "        \n",
    "        Arguments:\n",
    "        ----------------\n",
    "        pre_activations : pre activations (z) outputed by forward() function\n",
    "        y_true          : The ground truth y values\n",
    "        y_pred          : The outputs calculated by this neural network object\n",
    "        dis_cache       : Leave as None\n",
    "        \n",
    "        Working 2:\n",
    "        [use this if this is not the topmost nn and outputs of this nn is consumed by another nn ]\n",
    "        [ so now you have to backpropagate from that top nn into this one ]\n",
    "        \n",
    "        Arguments:\n",
    "        ----------------\n",
    "        pre_activations : pre activations (z) outputed by forward() function\n",
    "        y_true          : Leave as None\n",
    "        y_pred          : Leave as None\n",
    "        dis_cache       : a dicionary with keys:\n",
    "                          'weights0': the weights of the input layer of nn just on top of this one\n",
    "                          'deltas0' : the deltas of the input layer of the nn just on top of this one, \n",
    "                                      used to backpropagate into this nn\n",
    "        returns: \n",
    "        ----------------\n",
    "        deltas of all nodes this nn, dis_cache you can use to backprop into any nn whose outputs this one gets as inputs.\n",
    "        \n",
    "        '''\n",
    "        # initialize array to store the derivatives\n",
    "        deltas = [0] * (len(self.size) - 1)\n",
    "\n",
    "\n",
    "        # Calculate the delta for each layer\n",
    "        # This is the first step in calculating the derivative\n",
    "        # The last layer is calculated as derivative of cost function *  derivative of sigmoid ( pre-activations of last layer )\n",
    "        if dis_cache is None:\n",
    "            deltas[-1] = cross_entropy_prime(y_true, y_pred) * activation(pre_activations[-1], a_type= self.out_activation, derivative=True) \n",
    "        \n",
    "        # If this nn has to backpropagate from some other nn on top of this one:\n",
    "        else:\n",
    "            deltas[-1] = np.dot(dis_cache['weights0'].T, dis_cache['deltas0']) * activation(pre_activations[-1], a_type= self.out_activation, derivative=True)\n",
    "        \n",
    "\n",
    "        # Recursively calculate delta for each layer from the previous layer\n",
    "        for l in range(len(deltas) - 2, -1, -1):\n",
    "            # deltas of layer l depend on the weights of layer l and l+1 and on the sigmoid derivative of the pre-activations of layer l            # Note that we use a dot product when multipying the weights and the deltas\n",
    "            # Check their shapes to ensure that their shapes conform to the requiremnts (You may need to transpose some of the matrices)\n",
    "            # The final shape of deltas of layer l must be the same as that of the activations of layer l\n",
    "            # Check if this is true\n",
    "            deltas[l] = np.dot(self.weights[l+1].T,deltas[l+1]) * activation(pre_activations[l], a_type=self.activation, derivative=True)\n",
    "        \n",
    "        # make the cache\n",
    "        cache = {\n",
    "            'weights0': self.weights[0],\n",
    "            'deltas0': deltas[0]\n",
    "        }\n",
    "        \n",
    "        return deltas, cache\n",
    "    \n",
    "    \n",
    "    def backpropagate(self, deltas, pre_activations, activations):\n",
    "    \n",
    "        '''\n",
    "        This function is used to calculate the derivatives of all the weights and biases from the deltas\n",
    "        \n",
    "        \n",
    "        Arguments:\n",
    "        --------------------\n",
    "        deltas          : The deltas of each node calculated by compute_deltas\n",
    "        pre_activations : The pre-activations (z) calculated in the forward propagation\n",
    "        activations     : The activations (a) calculated in the forward propagation\n",
    "        \n",
    "        returns:\n",
    "        --------------------\n",
    "        The derivatives of all the weights and biases\n",
    "        '''\n",
    "        \n",
    "        # initialize the derivatives:\n",
    "        dW = []\n",
    "        db = []\n",
    "        deltas = [0] + deltas\n",
    "        \n",
    "        # go through all the layers!\n",
    "        for l in range(1, len(self.size)):\n",
    "            dW_temp = np.dot(deltas[l], activations[l-1].T)\n",
    "            db_temp = deltas[l]\n",
    "            dW.append(dW_temp)\n",
    "            db.append(np.expand_dims(db_temp.mean(axis=1), 1))\n",
    "        \n",
    "        \n",
    "        return dW, db\n",
    "    \n",
    "    \n",
    "    def train(self, X, y=None, learning_rate=0.01, dis_cache=None, should_print = True, print_acc=True, learn=True):        \n",
    "        \n",
    "        '''\n",
    "        Use this function to train the nn!!!\n",
    "        \n",
    "        \n",
    "        Working 1:\n",
    "        [use this if this is the topmost nn]\n",
    "        \n",
    "        Arguments:\n",
    "        -------------------\n",
    "        [NOTE]: The data is accepted with number of examples as index 2, not the first index like in tensorflow!!\n",
    "        \n",
    "        X             : The input of the training data\n",
    "        y             : The output of the training data\n",
    "        dis_cache     : Leave as None\n",
    "        learning_rate : The learning rate for the gradient desent\n",
    "        should_print  : True if you want this nn to print it's performance wrt to y_true, else pass False\n",
    "        print_acc     : If True it prints the accuracy while printing the performance\n",
    "        learn         : True if you want this nn to learn from the training data, else False\n",
    "        \n",
    "        \n",
    "        Working 2:\n",
    "        [use this if this is not the topmost nn and outputs of this nn is consumed by another nn ]\n",
    "        [ so now you have to backpropagate from that top nn into this one ]\n",
    "        \n",
    "        Arguments:\n",
    "        -------------------\n",
    "        [NOTE]: The data is accepted with number of examples as index 2, not the first index like in tensorflow!!\n",
    "        \n",
    "        X             : The input of the training data\n",
    "        y             : leave as None\n",
    "        dis_cache     : the cache from training of the nn, just on top of this one (used for backpropagation)\n",
    "        learning_rate : The learning rate for the gradient desent\n",
    "        should_print  : True if you want this nn to print it's performance wrt to y_true, else pass False\n",
    "        print_acc     : If True it prints the accuracy while printing the performance\n",
    "        learn         : True if you want this nn to learn from the training data, else False\n",
    "        \n",
    "        \n",
    "        returns:\n",
    "        -------------------\n",
    "        history (dictionary containing the training loss and training accuacy *** before training ***), \n",
    "        cache from compute deltas\n",
    "        '''\n",
    "        \n",
    "        # rename the variables for more readable code!\n",
    "        x_train = X\n",
    "        y_train = y\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        calculate the mertics and show how good the preformance is before training:\n",
    "        '''\n",
    "        \n",
    "        history = None\n",
    "        if y is not None:\n",
    "            # calculate metrics on training data\n",
    "            y_train_pred = self.calc_output(x_train)\n",
    "            train_loss = cross_entropy_loss(y_train, y_train_pred)\n",
    "            train_accuracy = accuracy(np.argmax(y_train, axis=0), np.argmax(y_train_pred, axis=0))\n",
    "            \n",
    "            history = {'train_loss': train_loss,\n",
    "                       'train_acc': train_accuracy,\n",
    "                       }\n",
    "            \n",
    "            # print\n",
    "            if should_print:\n",
    "                if print_acc:\n",
    "                    print('[INFO] train loss: {} | train accuracy: {}'.format(\n",
    "                            '%0.3f'%np.mean(train_loss), '%0.3f'%train_accuracy))\n",
    "                else:\n",
    "                    print('[INFO] train loss: {} '.format('%0.3f'%np.mean(train_loss)))\n",
    "                    \n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Train the model!!::\n",
    "        '''               \n",
    "        y_pred, pre_activations, activations = self.forward(x_train)\n",
    "        if dis_cache is not None:\n",
    "            # if you backprop from another nn on top. Follow this:\n",
    "            deltas, cache = self.compute_deltas(pre_activations, dis_cache=dis_cache )\n",
    "        else:\n",
    "            # If this is the topmost nn:\n",
    "            deltas, cache = self.compute_deltas(pre_activations, y_true=y_train, y_pred=y_pred)\n",
    "        dW, db = self.backpropagate(deltas, pre_activations, activations)\n",
    "        \n",
    "        \n",
    "        # update time!!!!\n",
    "        if learn: \n",
    "            for i in range(len(self.weights)):\n",
    "                self.weights[i] -= learning_rate*dW[i]\n",
    "                self.biases[i] -= learning_rate*db[i]\n",
    "                \n",
    "        return history, cache\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if nn works:\n",
    "\n",
    "To do this, we will use the NeuralNetwork class to make a classifier that classifies these 28x28 MNIST images into it's categories (0 to 9).\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"readme_images/MnistExamples.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "X train: (60000, 784)\n",
      "y train: (60000, 10)\n",
      "X test : (10000, 784)\n",
      "y test : (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "X_train        = np.load('data/train_images.npy')\n",
    "y_train_labels = np.load('data/train_labels.npy')\n",
    "X_test         = np.load('data/test_images.npy')\n",
    "y_test_labels  = np.load('data/test_labels.npy')\n",
    "\n",
    "# convert to onehot vector\n",
    "y_train = np.eye(10)[y_train_labels.astype(int).flatten()]\n",
    "y_test = np.eye(10)[y_test_labels.astype(int).flatten()]\n",
    "\n",
    "# normalize\n",
    "# mean = np.mean(X_train)\n",
    "# std = np.std(X_train)\n",
    "# X_train = (X_train - mean)/std\n",
    "# X_test = (X_test - mean)/std\n",
    "\n",
    "# [or]\n",
    "\n",
    "# Scale\n",
    "X_train = X_train/255\n",
    "\n",
    "'''\n",
    "Here we choose scaling instead of normalization because,\n",
    "We will be using this same dataset for the gan!\n",
    "If we normalize the data, we will have inputs in a unbounded range,\n",
    "but if we only scale, we know the outputs will be [0, 1] and,\n",
    "we can achieve pixels of this range as outputs with a sigmoid out activation easily.'''\n",
    "\n",
    "# understand the shape of the dataset\n",
    "print('Shapes:')\n",
    "print('X train:', X_train.shape)\n",
    "print('y train:', y_train.shape)\n",
    "print('X test :', X_test.shape)\n",
    "print('y test :', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# look through the x and y of the dataset!\n",
    "EXAMPLE_NO = 7\n",
    "\n",
    "pyplot.imshow(X_train[EXAMPLE_NO].reshape(28, 28), cmap='gray_r')\n",
    "pyplot.show()\n",
    "print('y: ', y_train[EXAMPLE_NO])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] initializing network\n",
      "[INFO] Done!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize the neural network!\n",
    "nn = NeuralNetwork([784, 10], activation = 'relu', out_activation='sigmoid')\n",
    "print('[INFO] Done!!')\n",
    "\n",
    "'''\n",
    "Notice how small this nn is! Zero hidden layers and still it works with > 90% accuracy!!\n",
    "(if you want more accuracy add some hidden layers, but we are not solving classification now, \n",
    "we just want to see if this nn class works)\n",
    "'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] train loss: 0.700 | train accuracy: 0.047\n",
      "[INFO] test  loss: 1.545 | test  accuracy: 0.148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\pmicc laptop\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\ipykernel_launcher.py:32: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] train loss: 0.079 | train accuracy: 0.938\n",
      "[INFO] test  loss: 2.620 | test  accuracy: 0.885\n",
      "[INFO] train loss: 0.071 | train accuracy: 0.945\n",
      "[INFO] test  loss: 2.657 | test  accuracy: 0.895\n",
      "[INFO] train loss: 0.067 | train accuracy: 0.945\n",
      "[INFO] test  loss: 2.669 | test  accuracy: 0.898\n",
      "[INFO] train loss: 0.065 | train accuracy: 0.938\n",
      "[INFO] test  loss: 2.676 | test  accuracy: 0.901\n",
      "[INFO] train loss: 0.064 | train accuracy: 0.938\n",
      "[INFO] test  loss: 2.682 | test  accuracy: 0.903\n",
      "[INFO] train loss: 0.063 | train accuracy: 0.938\n",
      "[INFO] test  loss: 2.686 | test  accuracy: 0.904\n",
      "\n",
      "Done!!!\n"
     ]
    }
   ],
   "source": [
    "# define the batch size and number of epochs\n",
    "BATCH_SIZE = 128\n",
    "N_EPOCHS = 7\n",
    "\n",
    "# iterate through n epochs\n",
    "for e in range(N_EPOCHS):\n",
    "    for b in range(int(60000/BATCH_SIZE)):\n",
    "        \n",
    "        # calculate when to show the metrics to the user!\n",
    "        if b == 0:\n",
    "            out = True\n",
    "        else:\n",
    "            out = False\n",
    "        \n",
    "        # slice out the batch\n",
    "        X_train_batch = X_train[b*BATCH_SIZE: b*BATCH_SIZE+BATCH_SIZE].T\n",
    "        y_train_batch = y_train[b*BATCH_SIZE: b*BATCH_SIZE+BATCH_SIZE].T\n",
    "        \n",
    "        # Training Time!!!!!!\n",
    "        nn.train( X_train_batch, y_train_batch, learning_rate=0.001, should_print = out)\n",
    "        \n",
    "        # calculate the accuracy on the test set:\n",
    "        if out:\n",
    "            y_test_pred = nn.calc_output(X_test.T)\n",
    "            test_loss = cross_entropy_loss(y_test, y_test_pred)\n",
    "            test_accuracy = accuracy(np.argmax(y_test.T, axis=0), np.argmax(y_test_pred, axis=0))\n",
    "            print('[INFO] test  loss: {} | test  accuracy: {}'.format(\n",
    "                        '%0.3f'%np.mean(test_loss), '%0.3f'%test_accuracy))\n",
    "print()\n",
    "print('Done!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "# Stacked NN\n",
    "Before training a gan, Let's understand how to train two neural networks that are stacked on top of each other. We'll do it on the same MNIST dataset to avoid any confusions.\n",
    "\n",
    "To do this let's train a stacked nn where one nn (nn2) that gives us the final output (the class of the image), gets it's input from the output of another nn (nn1)! This nn, (nn1) is the one that takes the actual input ( the pixel values )!\n",
    "\n",
    "This will help us understand how we can backpropagate from one nn through a second one!\n",
    "\n",
    "NOTE: for the out activation of nn1 use only sigmoid or tanh, Don't use relu or leakyrelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] initializing network\n",
      "[INFO] initializing network\n",
      "[INFO] Done!!\n"
     ]
    }
   ],
   "source": [
    "nn1 = NeuralNetwork([784, 256, 64], activation = 'leakyrelu', out_activation='tanh')\n",
    "nn2 = NeuralNetwork([64, 16, 10], activation = 'leakyrelu', out_activation='sigmoid')\n",
    "print('[INFO] Done!!')\n",
    "\n",
    "# just to show how good this works we took a relatively deep nn ( if we see nn1 and nn2 combined)\n",
    "# but even this works really good with both nn1 and nn2 not having even 1 hidden layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] train loss: 0.676 | train accuracy: 0.102\n",
      "[INFO] test  loss: 0.537 | test  accuracy: 0.141\n",
      "[INFO] train loss: 0.031 | train accuracy: 0.961\n",
      "[INFO] test  loss: 1.287 | test  accuracy: 0.931\n",
      "[INFO] train loss: 0.020 | train accuracy: 0.977\n",
      "[INFO] test  loss: 1.470 | test  accuracy: 0.952\n",
      "[INFO] train loss: 0.018 | train accuracy: 0.977\n",
      "[INFO] test  loss: 1.582 | test  accuracy: 0.961\n",
      "[INFO] train loss: 0.016 | train accuracy: 0.977\n",
      "[INFO] test  loss: 1.674 | test  accuracy: 0.965\n",
      "\n",
      "Done!!!\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "for e in range(5):\n",
    "    for b in range(int(60000/BATCH_SIZE)):\n",
    "        if b == 0:\n",
    "            out = True\n",
    "        else:\n",
    "            out = False\n",
    "        \n",
    "        X_train_nn1   = X_train[b*BATCH_SIZE: b*BATCH_SIZE+BATCH_SIZE].T\n",
    "        X_train_nn2   = nn1.calc_output(X_train_nn1)\n",
    "        y_train_batch = y_train[b*BATCH_SIZE: b*BATCH_SIZE+BATCH_SIZE].T\n",
    "        \n",
    "        \n",
    "        # train nn2\n",
    "        _, cache = nn2.train(X_train_nn2, y_train_batch, learning_rate=0.001, should_print = out)\n",
    "        # backprop into nn1\n",
    "        nn1.train( X_train_nn1, dis_cache=cache, learning_rate=0.001, should_print = out)\n",
    "        \n",
    "        if out:\n",
    "            nn1_out = nn1.calc_output(X_test.T)\n",
    "            y_test_pred = nn2.calc_output(nn1_out)\n",
    "            test_loss = cross_entropy_loss(y_test, y_test_pred)\n",
    "            test_accuracy = accuracy(np.argmax(y_test.T, axis=0), np.argmax(y_test_pred, axis=0))\n",
    "            print('[INFO] test  loss: {} | test  accuracy: {}'.format(\n",
    "                        '%0.3f'%np.mean(test_loss), '%0.3f'%test_accuracy))\n",
    "\n",
    "print()\n",
    "print('Done!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "# The Gan!!!! \n",
    "Atlast! yaay!!!!\n",
    "\n",
    "### The structure of a GAN:\n",
    "\n",
    "<img src=\"readme_images/gan.PNG\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions:\n",
    "\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    ''' \n",
    "    This function generates random latent feature vectors from a normal distribution\n",
    "    \n",
    "    Arguments:\n",
    "    -----------------\n",
    "    latent_dim : the number of latent features needed for generation of one image\n",
    "    n_samples  : the number or latent vectors needed\n",
    "    \n",
    "    Returns:\n",
    "    -----------------\n",
    "    n_samples amount of latent vectors\n",
    "    '''\n",
    "    \n",
    "    # generate points in the latent space\n",
    "    return np.random.randn(latent_dim, n_samples) \n",
    "\n",
    "def generate_and_save_images(e):\n",
    "    \n",
    "    '''\n",
    "    use the generator to generate and store its results!!\n",
    "    \n",
    "    Arguments:\n",
    "    e: name of the image to be saved ( can be a string or epoch number)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # generate 50 images\n",
    "    latent_features = np.random.randn(N_LATENT_FEATURES, 50)\n",
    "    generated_images = generator.calc_output(latent_features)\n",
    "\n",
    "    for i in range(50):\n",
    "        # define subplot\n",
    "        pyplot.subplot(5, 10, 1 + i)\n",
    "        # turn off axis\n",
    "        pyplot.axis('off')\n",
    "        # plot raw pixel data\n",
    "        pyplot.imshow(generated_images[:, i].reshape(28, 28), cmap='gray_r')\n",
    "        \n",
    "    # save plot to file\n",
    "    cwd = os.getcwd()\n",
    "    if type(e) == str:\n",
    "        filename = os.path.join(cwd, 'outputs','1',e+'.png')\n",
    "    else:\n",
    "        filename = os.path.join(cwd, 'outputs','1',str(e)+'.png')\n",
    "    pyplot.savefig(filename)\n",
    "    pyplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] initializing network\n",
      "[INFO] initializing network\n",
      "[INFO] Done!!\n"
     ]
    }
   ],
   "source": [
    "# Note we can also interpret the 'noise' vector as a latent feature space!\n",
    "\n",
    "# define the nn structure\n",
    "N_LATENT_FEATURES = 10\n",
    "N_IMAGE = 784\n",
    "\n",
    "# initialize the two neural networks:\n",
    "discriminator = NeuralNetwork([N_IMAGE, 32, 8, 1],                 activation = 'leakyrelu', out_activation='sigmoid')\n",
    "generator     = NeuralNetwork([N_LATENT_FEATURES, 64, N_IMAGE], activation = 'leakyrelu', out_activation='sigmoid')\n",
    "\n",
    "print('[INFO] Done!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] train loss: 0.883 \n",
      "[INFO] train loss: 0.372 \n",
      "[INFO] train loss: 0.363 \n",
      "[INFO] train loss: 0.347 \n",
      "[INFO] train loss: 0.355 \n",
      "[INFO] train loss: 0.395 \n",
      "[INFO] train loss: 0.340 \n",
      "[INFO] train loss: 0.399 \n",
      "[INFO] train loss: 0.446 \n",
      "[INFO] train loss: 0.493 \n",
      "[INFO] train loss: 0.507 \n",
      "[INFO] train loss: 0.405 \n",
      "[INFO] train loss: 0.433 \n",
      "[INFO] train loss: 0.453 \n",
      "[INFO] train loss: 0.379 \n",
      "[INFO] train loss: 0.462 \n",
      "[INFO] train loss: 0.408 \n",
      "[INFO] train loss: 0.346 \n",
      "[INFO] train loss: 0.420 \n",
      "[INFO] train loss: 0.443 \n",
      "[INFO] train loss: 0.465 \n",
      "[INFO] train loss: 0.438 \n",
      "[INFO] train loss: 0.477 \n",
      "[INFO] train loss: 0.434 \n",
      "[INFO] train loss: 0.427 \n",
      "Done!!!\n"
     ]
    }
   ],
   "source": [
    "# Now train the GAN!!!\n",
    "\n",
    "BATCH_SIZE  = 128\n",
    "HALF_BATCH = int(BATCH_SIZE/2)\n",
    "\n",
    "# initialize stuff:\n",
    "discriminator_loss = 0.6\n",
    "\n",
    "# generate images from the untrained model:\n",
    "generate_and_save_images('initial')\n",
    "\n",
    "for e in range(25):\n",
    "    for b in range(int(60000/HALF_BATCH)):\n",
    "        \n",
    "        # decide when to print:\n",
    "        if b == 0:\n",
    "            out = True\n",
    "        else:\n",
    "            out = False            \n",
    "        \n",
    "        \n",
    "        # decide how to learn:\n",
    "        '''\n",
    "        If discriminator is too strong train only the generator, and vice versa.\n",
    "        On the other hand if none of them overpowers the other, train both!'''\n",
    "        if discriminator_loss > 0.8:\n",
    "            # generator has grown too powerful!!!\n",
    "            d_learn = True\n",
    "            g_learn = False\n",
    "            \n",
    "        elif discriminator_loss < 0.3:\n",
    "            # discriminator has grown too powerful!!!\n",
    "            d_learn = False\n",
    "            g_learn = True\n",
    "            \n",
    "        else:\n",
    "            # they are well balanced :)\n",
    "            d_learn = True\n",
    "            g_learn = True\n",
    "            \n",
    "            \n",
    "        '''\n",
    "        Train the discriminator\n",
    "        '''\n",
    "        # real data:\n",
    "        X_train_real = X_train[b*HALF_BATCH: b*HALF_BATCH+HALF_BATCH].T\n",
    "        y_train_real = np.ones((1, HALF_BATCH))\n",
    "\n",
    "        # fake data:\n",
    "        latent_features = generate_latent_points(N_LATENT_FEATURES, HALF_BATCH)\n",
    "        X_train_generated = generator.calc_output(latent_features)\n",
    "        y_train_generated = np.zeros((1, HALF_BATCH))\n",
    "\n",
    "        # complete training set for discriminator:\n",
    "        X_train_batch = np.append(X_train_generated, X_train_real, axis=1)\n",
    "        y_train_batch = np.append(y_train_generated, y_train_real, axis=1)\n",
    "\n",
    "        # train the discriminator:\n",
    "        history, _ = discriminator.train(X_train_batch, y_train_batch, learning_rate=0.001, should_print = out, print_acc=False, learn=d_learn)\n",
    "\n",
    "        discriminator_loss = np.mean(history['train_loss'])\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Train the generator\n",
    "        '''\n",
    "        # create latent features and y:\n",
    "        latent_features         = generate_latent_points(N_LATENT_FEATURES, BATCH_SIZE)\n",
    "        X_train_generated_batch = generator.calc_output(latent_features)\n",
    "        y_train_batch           = np.ones((1, BATCH_SIZE)) \n",
    "        \n",
    "            # the discriminator is trained to say 1 to real data and zero to fake ones.\n",
    "            # but to this we pass all ones even though we pass fake images because we\n",
    "            # want the generator to learn to make the discriminator say this false prediction!\n",
    "        \n",
    "        # forward and backprop on discriminator:\n",
    "        # Note: Donot learn here!\n",
    "        _, cache = discriminator.train(X_train_generated_batch, y_train_batch, learning_rate=0.001, should_print = False, learn=False)\n",
    "        # backprop into generator\n",
    "        generator.train( latent_features, dis_cache=cache, learning_rate=0.001, should_print = False, learn=g_learn)\n",
    "        \n",
    "        if out:\n",
    "            generate_and_save_images(e)\n",
    "\n",
    "generate_and_save_images('final')\n",
    "print('Done!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAANYklEQVR4nO3db6xU9Z3H8c8HFiRCo7BcES3x1saENZu70Iy4xk3jhlj/PME+aFMeNGxiSk0wttqYNe6D+pBsbJuabBrpSko3XUlNayTG7NaQJobENHc0CLg3XV1lWwrCBaMFRVD57oN72FzhzpnLnDlzBr7vVzKZmfOdM+fLcD/3zJ3fOfNzRAjApW9O0w0AGAzCDiRB2IEkCDuQBGEHkviLQW5s6dKlMTo6OshNAqns379fR48e9Uy1SmG3faekH0uaK+lfI2Jz2eNHR0fVbrerbBJAiVar1bHW89t423Ml/YukuyTdKGm97Rt7fT4A9aryN/saSW9GxFsRcVrSdknr+tMWgH6rEvZrJf1x2v0DxbLPsL3Rdtt2e3JyssLmAFRRJewzfQhw3rG3EbElIloR0RoZGamwOQBVVAn7AUkrpt3/vKSD1doBUJcqYR+XdIPtL9ieL+kbknb0py0A/dbz0FtEfGL7fkn/qamht60R8XrfOgPQV5XG2SPiBUkv9KkXADXicFkgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEpWmbLa9X9JxSZ9K+iQiWv1oCkD/VQp74e8j4mgfngdAjXgbDyRRNewh6Te2X7G9caYH2N5ou227PTk5WXFzAHpVNey3RsSXJN0laZPtL5/7gIjYEhGtiGiNjIxU3ByAXlUKe0QcLK6PSHpW0pp+NAWg/3oOu+2Ftj939rakr0ja16/GAPRXlU/jl0l61vbZ5/n3iPiPvnSVzKlTp0rrl1122YA6Gax9+8r3DWvXri2tHzx4sLQ+d+7cC+7pUtZz2CPiLUl/08deANSIoTcgCcIOJEHYgSQIO5AEYQeS6MeJMOgiIkrr3YbWzpw5U1ofGxvrWJuYmChdt9vw1Mcff1xar2J8fLy03m1o7cknnyyt33fffR1rc+bk28/l+xcDSRF2IAnCDiRB2IEkCDuQBGEHkiDsQBKMsw/AyZMnS+sLFy4src+fP7+0fvr06Qvu6axuY/hVlf3bbr/99tJ1jx07VlrfsGFDab3sGIJuxz5citizA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASjLMPwOWXX15p/Srj6A899FBpfd68eaX1buPwmzdvLq1XOW+8+Jryjh544IGen/vDDz8srVf9PxtG7NmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnG2Qfg8ccfr/X5jx8/3rG2aNGiWrddpwULFpTWn3jiiZ6fO+N0zl337La32j5ie9+0ZUtsv2j7jeJ6cb1tAqhqNm/jfybpznOWPSJpZ0TcIGlncR/AEOsa9oh4SdK75yxeJ2lbcXubpHv63BeAPuv1A7plEXFIkorrqzo90PZG223b7cnJyR43B6Cq2j+Nj4gtEdGKiNbIyEjdmwPQQa9hP2x7uSQV10f61xKAOvQa9h2Szn6P7wZJz/WnHQB16TrObvtpSbdJWmr7gKTvS9os6Ze275X0B0lfq7PJi93DDz9c6/NfrGPp119/fWn9o48+qm3bGb83vmvYI2J9h9LaPvcCoEYcLgskQdiBJAg7kARhB5Ig7EASnOJ6EVi2bFnTLdTi7bffbmzb3U6fvRSxZweSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJBhn74O6x8G7TZs8zLpNu1yn0dHRxrY9jNizA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASjLP3wdVXX11af//990vrp06dKq13mzbrgw8+6Fg7evRo6brXXXddaX337t2l9dWrV5fWm7R169amWxgq7NmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnG2Wfp5Zdf7ljrNo4+NjZWWn/ttddK66dPny6tV5myudv55k1ObTx//vzSerfjE/BZXffstrfaPmJ737Rlj9n+k+3dxeXuetsEUNVs3sb/TNKdMyz/UUSsKi4v9LctAP3WNewR8ZKkdwfQC4AaVfmA7n7be4q3+Ys7Pcj2Rttt2+1ux3gDqE+vYf+JpC9KWiXpkKQfdHpgRGyJiFZEtEZGRnrcHICqegp7RByOiE8j4oykn0pa09+2APRbT2G3vXza3a9K2tfpsQCGQ9dxdttPS7pN0lLbByR9X9JttldJCkn7JX27xh6Hwi233NKxNj4+XrrusWPHSusrV64srZedry5Ju3bt6li74447Stddv359aX379u2l9Tpt2rSpsW1firqGPSJm+ml4qoZeANSIw2WBJAg7kARhB5Ig7EAShB1IwoM8hbHVakW73R7Y9tDde++9V1q/5pprSusnT57sedvz5s0rrXc7tRfna7VaarfbM563zJ4dSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Lgq6STe/DBB0vrVcbRu3nnnXdqe26cjz07kARhB5Ig7EAShB1IgrADSRB2IAnCDiTBOHty27Ztq/X5FyxY0LG2ZMmSWreNz2LPDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJMM6eXN3zBtR5PjwuTNc9u+0Vtn9re8L267a/UyxfYvtF228U14vrbxdAr2bzNv4TSd+LiL+S9LeSNtm+UdIjknZGxA2Sdhb3AQyprmGPiEMR8Wpx+7ikCUnXSlon6eyxltsk3VNXkwCqu6AP6GyPSlot6XeSlkXEIWnqF4Kkqzqss9F223Z7cnKyWrcAejbrsNteJOlXkr4bEX+e7XoRsSUiWhHRGhkZ6aVHAH0wq7DbnqepoP8iIn5dLD5se3lRXy7pSD0tAuiHrkNvti3pKUkTEfHDaaUdkjZI2lxcP1dLh6hk7969TbeAITGbcfZbJX1T0l7bu4tlj2oq5L+0fa+kP0j6Wj0tAuiHrmGPiF2SZpzcXdLa/rYDoC4cLgskQdiBJAg7kARhB5Ig7EASnOJ6iVu1alWtzz8xMVHr86N/2LMDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKMs1/iTpw4UVq/8sorS+tXXHFFaX3lypUX3BOawZ4dSIKwA0kQdiAJwg4kQdiBJAg7kARhB5JgnP0SsGfPno61Z555pnTdOXPKf9/v2LGjp54wfNizA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASs5mffYWkn0u6WtIZSVsi4se2H5P0LUmTxUMfjYgX6moUnY2NjXWs3XTTTaXrPv/886X1m2++uaeeMHxmc1DNJ5K+FxGv2v6cpFdsv1jUfhQRj9fXHoB+mc387IckHSpuH7c9IenauhsD0F8X9De77VFJqyX9rlh0v+09trfaXtxhnY2227bbk5OTMz0EwADMOuy2F0n6laTvRsSfJf1E0hclrdLUnv8HM60XEVsiohURrZGRkT60DKAXswq77XmaCvovIuLXkhQRhyPi04g4I+mnktbU1yaAqrqG3bYlPSVpIiJ+OG358mkP+6qkff1vD0C/zObT+FslfVPSXtu7i2WPSlpve5WkkLRf0rdr6RA6c+ZMab3sNNXx8fHSdcuG7XBpmc2n8bskeYYSY+rARYQj6IAkCDuQBGEHkiDsQBKEHUiCsANJ8FXSF4FuX/dchnF0nMWeHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeScEQMbmP2pKT/nbZoqaSjA2vgwgxrb8Pal0Rvvepnb9dFxIzf/zbQsJ+3cbsdEa3GGigxrL0Na18SvfVqUL3xNh5IgrADSTQd9i0Nb7/MsPY2rH1J9NargfTW6N/sAAan6T07gAEh7EASjYTd9p22f2/7TduPNNFDJ7b3295re7ftdsO9bLV9xPa+acuW2H7R9hvF9Yxz7DXU22O2/1S8drtt391Qbyts/9b2hO3XbX+nWN7oa1fS10Bet4H/zW57rqT/lnS7pAOSxiWtj4j/GmgjHdjeL6kVEY0fgGH7y5JOSPp5RPx1seyfJb0bEZuLX5SLI+Ifh6S3xySdaHoa72K2ouXTpxmXdI+kf1CDr11JX1/XAF63JvbsayS9GRFvRcRpSdslrWugj6EXES9JevecxeskbStub9PUD8vAdehtKETEoYh4tbh9XNLZacYbfe1K+hqIJsJ+raQ/Trt/QMM133tI+o3tV2xvbLqZGSyLiEPS1A+PpKsa7udcXafxHqRzphkfmteul+nPq2oi7DNNJTVM43+3RsSXJN0laVPxdhWzM6tpvAdlhmnGh0Kv059X1UTYD0haMe3+5yUdbKCPGUXEweL6iKRnNXxTUR8+O4NucX2k4X7+3zBN4z3TNOMagteuyenPmwj7uKQbbH/B9nxJ35C0o4E+zmN7YfHBiWwvlPQVDd9U1DskbShub5D0XIO9fMawTOPdaZpxNfzaNT79eUQM/CLpbk19Iv8/kv6piR469HW9pNeKy+tN9ybpaU29rftYU++I7pX0l5J2SnqjuF4yRL39m6S9kvZoKljLG+rt7zT1p+EeSbuLy91Nv3YlfQ3kdeNwWSAJjqADkiDsQBKEHUiCsANJEHYgCcIOJEHYgST+D8YmC0O5gHy3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lets generate some examples:\n",
    "latent_features = np.random.randn(N_LATENT_FEATURES, 1)\n",
    "generated_image = generator.calc_output(latent_features)\n",
    "\n",
    "pyplot.imshow(generated_image.reshape(28, 28), cmap='gray_r')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yaay!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results:\n",
    "\n",
    "The gan has been trained! But it was trained only on a cpu (since we used only numpy package and did not do any cuda coding ) and we trained it only for 25 epochs (~ 30 mins in a laptop 4 core i5 cpu ). We can improve the performance by, making the network bigger, training it for longer etc. But if we want to improve our model a lot, what we can do is use a CNN instead of a vanilla fully connected RNN!\n",
    "\n",
    "### epoch 0:\n",
    "<img src=\"outputs/1/initial.png\" />\n",
    "\n",
    "### epoch 5:\n",
    "<img src=\"outputs/1/5.png\" />\n",
    "\n",
    "### final random samples:\n",
    "<img src=\"outputs/1/final.png\" />\n",
    "\n",
    "## Good examples underlined:\n",
    "\n",
    "<img src=\"readme_images/results.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"readme_images/the_end.jpg\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
